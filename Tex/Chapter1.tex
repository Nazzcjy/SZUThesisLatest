\chapter{绪论}\label{chap:intro}

\section{研究背景}\label{sec:background}
随着信息技术和互联网媒体的崛起，如博客、维基百科、社交媒体平台等，文本数据已经成为当代社会信息传播的重要载体。其中，短文本作为信息传播的一种高效形式，其数量在互联网时代经历了爆炸性的增长。短文本通常指的是字数较少、内容简洁的文本数据。它们的主要特点是信息量密集，但表达形式极为简洁。比如在社交平台中，不论是用户发表的微博和小红书，还是标题、弹幕以及评论等，绝大多数都以短文本的形式存在。由于短文本在个人日常交流、商业广告、新闻报道等领域扮演着重要的角色，对短文本进行分析研究不仅对于理解和挖掘网络社会的信息动态具有重要意义，也对于商业智能和公共管理等领域的决策支持具有实际价值。

目前，主题模型仍是一种高效挖掘短文本数据集的常用算法。主题挖掘能够自动发现文本中隐藏的主题，是为了文本信息提取这一任务所设计的一类无监督机器学习技术，使得人们能够更快速且全面地理解文本所包含的内容。该方法通过对文档语料库进行统计分析，能够将包含大量文档的语料库压缩成一个简短的摘要，以揭示语料库中的潜在主题。这个简短的摘要采用主题的形式，即一组相关的词语，因此被称为主题模型。同时，每个文档可以被表示为在这些潜在主题上的一个分布。这种“文档-主题-词”结构提供了语义可解释性，使我们能够更好地理解语料库所包含的主题信息。

基于主题模型的有效性和可解释性，短文本主题挖掘在现实生活中得到了广泛的应用。例如，在商业领域，通过分析消费者的短评和反馈，能够及时捕捉市场趋势和消费者需求，为产品开发和市场营销提供指导。在社交媒体分析中，通过对大量用户发表的短文本进行主题挖掘，可以迅速了解公众情绪、舆论走向或热点话题。自动检测社交媒体上的事件讨论，这在许多不同的领域中都有用途。例如，在国家安全行动中用于预测骚乱并检测虚假信息\cite{DisruptiveEventDetection}，而人为地监控社交媒体中的反社会行为和反事实行为是十分受限制的。利用主题模型，可以深入挖掘和利用海量短文本背后有价值的信息。
    
%由于主题模型的有效性和可解释性，主题挖掘在现实生活中得到了广泛的应用。举例来说，在医疗领域，电子健康记录提供了丰富的临床信息，使用主题模型可以提取流行病学模式以了解和预测患者的疾病风险\cite{PredictDisease}。另一个例子是自动检测社交媒体上的事件讨论，这在许多不同的领域中都有用途。例如，在国家安全行动中用于预测骚乱并检测虚假信息\cite{DisruptiveEventDetection}，而人为地监控社交媒体中的反社会行为和反事实行为是十分受限制的。此外，社交网络舆情已成为风险防范管理的重要情报源，从中检测及分析事件将成为实时了解业务风险和廉政风险动态，进而及时开展风险布控和纪检监察行动的重要渠道之一。
\subsection{短文本特征概述}
随着社交媒体和电商平台的不断发展，短文本已经成为当今互联网时代常见的一类文本。短文本的长度，少则几个词，多则十几个词，与常规长度的文本相比存在相当大的差别。传统的书籍和科学文章存在着具有辨识度的单词共现模式，而短文本可能拥有非常不同的模式\cite{survey_2023}。短文本数据，尤其是互联网中的短文本，具有如下的特征：（1）稀疏性：社交媒体短文本不同于编辑精良的文章，往往只包含少数匆忙输入的单词。因此短文本的长度十分有限。此外，社交媒体数据的词汇不断演变，新词和标签不断涌现。帖子中包含多种语言并不罕见，缩写更是常规。在社交媒体帖子中，相较于长文本，高频的单词共现模式几乎不存在。（2）语境缺失：由于篇幅限制，短文本往往缺乏足够的上下文信息来清晰地表达其含义。这使得理解和分析短文本的语义内容更加困难。（3）动态性和时效性：社交媒体等平台上的短文本往往与当前事件、趋势和公众关注点密切相关，展现出强烈的动态性和时效性。（4）潜在主题的复杂性：单个短文本所包含的信息有限，但文本与文本之间的联系可以揭示复杂且多样的主题信息。
% \subsection{短文本特征概述}
% 随着社交媒体和电商平台的不断发展，短文本已经成为当今互联网时代常见的一类文本。比如在社交平台中，不论是用户发表的微博和小红书，还是标题、弹幕以及评论等，绝大多数都是短文本。这些文本的长度，少则几个词，多则十几个词，和常规长度的文本相比存在相当大的差别。与常规文本不同，短文本数据，尤其是互联网中的短文本，具有如下的特征\cite{Evolution}：
%此外，传统的主题模型主要聚焦于常规文本的主题挖掘，如NMF\cite{NMF}（Non-negative Matrix Factorization）和LDA\cite{LDA}（Latent Dirichlet Allocation）等在发现学术文章等长文本的潜在主题结构上有着强大的能力\cite{StrongAbility}。这促使我们希望在短文本上实现类似的强大和准确的效果。然而，在对社交媒体等短文本语料进行分析时，传统主题模型的表现并不理想。

% \begin{itemize}
%     \item[(1)] 稀疏性。社交媒体短文本不同于编辑精良的文章，往往只包含少数匆忙输入的单词。因此短文本的长度十分有限。此外，社交媒体数据的词汇不断演变，新词和标签不断涌现。帖子中包含多种语言并不罕见，缩写更是常规。在社交媒体帖子中，相较于长文本，高频的单词共现模式几乎不存在。
%     \item[(2)] 数据量。不同于每年出版数千本书籍和研究论文，每天都有数百万条社交媒体帖子生成。许多最初的主题模型是建立在统计模型的基础上的，这些模型的推断方法会变得很棘手，往往需要一些简化才能适用于大量的文本分析。
%     \item[(3)] 变化速度。信息不再以年度、季度、月度甚至每周的频率发布，社交媒体促使信息实时发布。这意味着今天的主题可能与昨天不同，明天可能也不相同。
% \end{itemize}

\subsection{短文本主题模型的挑战}
由于主题模型往往依赖于文档层面的单词共现信息来推断潜在主题\cite{STTM}，但由于短文本噪声多、数据量大、文本长度短等特征，要找到短文本数据集中的词共现性信息是相当有挑战性的。即使是最先进的（针对常规文本的）主题模型，在应用于短文本时，往往也会挖掘出嘈杂甚至是无法解释的主题。因此，短文本主题模型的核心问题在于短文本可使用的单词共现信息相对较少。如果能够使模型利用更为充分和可靠的语义信息，就有望有效提升主题模型在短文本集上的效果。因此已有的研究工作大致可分成两个类别\cite{ZJHNY}，一类方法基于短文本的特性，引入先验知识以增加词语共现信息，从而缓解稀疏性问题。另一类方法借助外部知识引入除词语共现信息之外的语义信息，与词语共现信息在建模过程中形成互补。

第一类方法引入适用于短文本特性的假设来缓解稀疏性问题。这类方法通常利用对短文本集的先验知识，设计特定的模型以更好地适应短文本的特征。例如，考虑到短文本的长度有限，一个短文本的内容可能仅涉及有限的主题。利用这种先验知识设计模型，可以使主题模型更贴近短文本集的特性。然而，这些基于先验知识的主题模型主要依赖传统的统计推断方法，如变分推断和Gibbs采样。随着主题模型结构变得复杂，这些推断方法的复杂度显著增加。同时，在处理大规模文本集时，难以有效扩展或利用GPU等并行计算设备\cite{NTMsurvey}。

第二类方法引入外部语义信息，让模型能够同时使用词语共现信息和其他语义信息。以微博为例，元信息如标签、作者、地点和时间戳等可以用于将短文聚合成长文，再对将传统的主题模型应用于这些短文上。然而，由于元数据的有限外部来源，这些方法未能达到预期的结果。同时聚合策略会减少文档数量，进而导致后续模型在建模时会面临文档数量有限这一问题。另一种直观的想法是借助外部语料将短文本扩充为长文本。但是，短文本集和扩充后得到的文本集需要在语义上尽量保持一致，否则这一做法会引入噪声，让主题模型的结果更差\cite{ZJHNY}。

综上，尽管许多研究提出了各种短文本主题模型以应对短文本的稀疏性，但是这些方法仍存在着适用性的问题以及计算时间复杂度较大的问题。下面本文将更详细的介绍已有的研究工作。

\section{国内外研究现状}\label{sec:relatedWork}
国内外研究者从多个方面展开工作来解决短文本主题挖掘存在的问题，这些工作主要分为两大类：第一类方法是引入适用于短文本特性的假设，从而增加词共现信息对短文本进行建模。第二类方法引入外部语义信息，让模型能够同时使用词语共现信息和其他语义信息。

\subsection{引入适用于短文本特性的假设}
这类方法可以分成四种策略\cite{STTM,XSJMXQQ}，分别是基于狄利克雷多项混合（Dirichlet Multinomial Mixture，DMM）的模型，基于稀疏先验假设的模型，基于全局词共现信息的模型，以及基于文本自聚合的模型。

\textbf{（1）基于DMM的模型}

DMM\cite{DMM}模型最初Nigam等人由提出，已被广泛应用于推断短文本中的潜在主题。该模型基于一种简单的假设策略，即一个短文本仅包含一个潜在主题。相较于LDA模型假设一个文本包含多个主题，这更适用于短文本。DMM模型设计之初采用的是基于Expectation–Maximization（EM）的推断算法。而GSDMM\cite{GSDMM}提出了一个基于吉布斯采样（Gibbs Sampling）的DMM模型。许多方法都集中在利用词的嵌入表示来检索语义信息来进一步降低了稀疏度。PDMM\cite{PDMM}模型认为主题数应该服从于泊松分布，设计了一个基于泊松分布和词嵌入的DMM模型。GPU-DMM\cite{GPUDMM}模型和GPU-PDMM\cite{PDMM}模型基于广义波利亚罐（Generalized Pόlya Urn, GPU）模型和词嵌入，利用语义相似的词去提升DMM模型的采样过程。GPM\cite{GPM}采用伽马分布和泊松分布改进DMM，但在处理复杂短文本时性能有限。LF-DMM\cite{LFDMM}提出了基于潜在特征向量的DMM模型，通过改进特征词表示并引入词-主题映射来提高模型性能。Lap-DMM\cite{LAPDMM}提出了一种带有变分流形正则化的DMM主题模型，以提高主题分类准确率。然而，它基于文档之间的相似程度，具有一定的复杂性。MultiKE-DMM\cite{MKEDMM}模型利用知识图谱和词嵌入增强了DMM模型的采样过程。APU-DMM\cite{APUDMM}则通过自适应调整GPU-DMM的提升权重获得了更好的性能。

\textbf{（2）基于稀疏先验假设的模型}

稀疏主题模型对于主题数的假设区别于DMM和LDA模型。通常来说，每一个文本的内容应该关注在一小部分主题上，而不是仅仅一个主题或者所有的主题这样的极端情况。Dual-Sparse\cite{DualSparse}模型通过“Spike and Slab”\cite{SpikeSlab}先验限制短文本对应的主题数量以及每个主题包含的词汇数。稀疏主题编码\cite{STC}（STC）利用拉普拉斯先验直接控制文档主题分布的稀疏性。随着深度学习的发展，研究者们也将目光转向利用神经网络实现主题模型。NSTC\cite{NSTC}联合利用词嵌入和神经网络实现STC模型。Sparsemax-NVDM\cite{SparseMax,NVDM}利用变分自编码器\cite{VAE}（Variational AutoEncoder，VAE）和Sparsemax 激活函数构建主题模型，并且限制了一个短文本只能对应少数主题。CRNTM\cite{CRNTM}模型在神经网络主题模型的解码阶段用高斯分布引入了辅助数据集的词嵌入信息，并且利用贝塔分布限制了短文本对应的主题数量。DVAE.Sp\cite{DVAE}模型，直接利用Sigmoid激活函数函数选择与当前文本相关的主题。NQTM\cite{NQTM}模型则基于VQ-VAE\cite{VQVAE}（Vector Quantised Variational AutoEncoder）构建了稀疏的文档主题分布。TSCTM\cite{TSCTM}在NQTM的基础上加入了对比学习的思想以增强模型的表现。

\textbf{（3）基于全局词共现信息的模型}

为了解决词共现信息是不足的问题，一些模型尝试利用原始数据集中丰富的全局词共现信息来推断隐藏的主题。全局词共现可在一定程度上缓解短文本稀疏性问题。这些模型需要配置一个滑动窗口来提取词共现。这种类型的模型可以根据全局词共现的利用策略分为两类。第一类可以通过利用全局词共现信息推断潜在主题。例如，BTM\cite{BTM}模型假设构成一个双词的两个词具有相同的主题，这个主题是从整个数据集中的各种主题中得出的。而这两类中的第二类，比如WNTM\cite{WNTM}则基于全局词共现创建一个词共现网络，然后从构建的网络中找出隐藏的主题，其中每个单词都代表构建网络的一个节点。

然而，BTM可能会丢失一些在语料库中无法观察到的显著且连贯的词共现信息。它还容易受到噪声干扰，提取出许多不相关的双词。LS-BTM\cite{LS-BTM}利用潜在语义细节进行主题提取，并改善BTM的性能。然而，该模型使用了更多与主题不相关的双词。R-BTM\cite{R-BTM}通过使用词嵌入的相关词相似性列表来克服BTM模型连贯性消失的问题。NBTMWE\cite{NBTMWE}结合了来自外部语料库的噪声BTM和词嵌入技术，以改善主题的连贯性。UGTM\cite{UGTM}基于上下文数据的语义关系开发了用户图主题模型。这种方法在动态主题提取方面非常高效。GLTM\cite{GLTM}基于全局和局部词嵌入进行主题建模，该模型使用连续Skip-Gram模型与负采样的适当编码来训练全局嵌入，以获取局部词嵌入。CSTM是一种共同语义主题模型，通过使用单词来过滤短文本主题发现中的噪音。但是，这个模型在设置优先级和确定主题标签数量方面存在局限性。

\textbf{（4）基于文本自聚合的模型}

基于自聚合的模型增加了一个新的隐变量，长文本，然后构造了一个短文本－长文本－主题－词语的联合概率分布。只要让长文本的数量少于短文本的总数，就能够让长文本成为短文本的一个聚类。其好处是在主题推理过程中同时进行主题建模和文本自我聚类。SATM\cite{SATM}模型是最早提出的自聚合方法，它将每个短文视为隐藏的长篇伪文档的样本，并将其合并，使用吉布斯采样进行主题提取。优点是不依赖元数据或辅助信息，但却很容易过度拟合，而且计算成本也很高。为了提高SATM模型的性能，PTM\cite{PTM}模型和SPTM\cite{SPTM}模型提出了伪文档的概念，将短文隐性地结合起来，以解决数据稀少的问题。PYSTM\cite{PYSTM}模型通过狄利克雷过程采样伪长文本的数量，实现伪长文本数量的参数化。SenU-PTM\cite{SenU-PTM}根据词与词嵌入的语义相似性生成短语，并用新的词汇表对原始语料进行标注，接着根据原始文本和语义关系生成具有共现关系的意义单元。

\subsection{引入外部语义信息}
一些研究把目光转向了除词共现信息以外的语义信息。一种直观的方法是利用外部知识将短文本进行扩充变成长文本，从而增加单词共现信息。目前关于短文本主题建模的文档扩展方法，大多数侧重于扩展Twitter数据。ET-LDA\cite{ExTwitter}提出了基于推文作者或语料库词汇中的聚合方案。考虑到元信息，DLDA\cite{DualLDA}使用由推文中URL链接的网页作为元长文档，以识别推文中更好的主题。然而，在某些领域中可能无法获得有利的元数据。Pooling\cite{PoolTwitter}评估了四种推文汇总方案，以改善LDA的结果。DREx\cite{DREx}提出了共频扩展（CoFE）和基于分布表示的扩展（DREx），将短文本扩展为一个可观的伪文档。AOTM\cite{AOTM}提出了一种新的主题模型，用于从短的用户评论文本和普通文本中提取主题。通过考虑作者身份，AOTM为每个短文本的作者提供了一个概率分布，该分布覆盖了一系列仅由短文本示例化的主题。一些短文本主题模型就试图利用上下文语义信息，从而弥补稀疏的共现信息。SeaNMF\cite{SeaNMF}模型通过非负矩阵分解的方式， 把短文本集的词嵌入引入模型中。RLSeaNMF\cite{RLSeaNMF}模型构造了一个结合强化学习和SeaNMF的模型。COTM\cite{COTM}模型面向博客及其评论数据，将主题分成标准主题和非标准主题，把常规文本集的语义信息迁移到短文本集中去。ASTM\cite{ASTM}模型和AATM\cite{AATM}模型则引入辅助文本集的词语共现信息，并将模型与注意力机制结合起来。

\section{本文解决的主要问题}
近年来，许多研究针对主题模型提出了各种策略和方法来解决短文本数据的稀疏性问题。但是这些方法要么无法为主题模型提供充分且可靠的语义信息，要么存在适用性问题。在本论文中，我们主要解决了以下问题：

（1）为了给主题模型提供充分的语义信息，一种直接的方法是从短文本自身出发，对其进行扩充以实现数据增强的目的，从而增加文档层面的词共现信息。但现有的文本扩充方法往往无法保留文本原来的语义信息，即扩充之后的“伪”长文与原来的短文之间存在语义不一致的问题。我们的方法则可以保证这种语义一致性。此外，不同于抛弃原始短文本，只利用扩充文本的主题模型，我们的模型将短文本与扩充文本联合建模，既利用伪长文本的词共现信息，又注重原始短文本中的独特信息，实现了高质量的主题挖掘。

（2）在一个大的主题集合中，一个文本往往只涉及其中的小部分主题。稀疏主题模型针对这一事实，引入了额外的正则化约束项或稀疏先验分布，以增强短文本主题模型的表达能力。然而，这些主题模型在引入辅助变量时，往往以牺牲部分准确性为代价，假设隐变量的后验分布之间是无关的，以简化目标函数的推导过程，即平均场假设\cite{VI}。但在传统的非深度学习框架下，这些模型的推导和求解仍旧复杂。我们的模型则利用变分自编码器构建了稀疏增强的非均场主题模型，并且利用了非均值场假设对后验分布进行建模解决了上述的问题，从模型增强的角度实现了高质量的主题挖掘。

\section{论文主要研究内容}
\subsection{短文本主题模型的数据增强方法}
数据增强的目的是解决短文本的长度有限和词共现信息匮乏的问题。论文将从两个方面展开研究：首先是短文本扩充技术，旨在生成高质量的扩充文本，称为伪长文本。其次是研究短文本与伪长文本之间的联合建模方法，以提升短文本主题建模的有效性。具体的研究内容如下：

（1）\textbf{短文本扩充技术：}文本扩充是一种常用的数据增强方法，通过依据某个数据模型，从原始文本计算得到符合该模型的目标系统所需的文本数据。随着在主题建模任务中，文本扩充技术已被证明能显著提高模型性能。然而，在实际应用中，由于外部知识的有限性，可能难以得到高质量的文本数据以提升模型性能。为解决这一问题，本论文将研究基于大语言模型的短文本扩充技术，利用大语言模型强大的外部知识进行文本生成，从而获得与短文本语义尽可能一致的伪长文本数据。

（2）\textbf{短文本与伪长文本的联合建模：}以往的研究通常直接利用针对常规文本的主题模型对扩充后的文本进行主题建模。然而，文本生成过程中不可避免的会引入额外的主题信息，从而影响主题挖掘的效果。为了应对这个问题，本论文将研究一种短文本与伪长文本联合的方法，既利用伪长文本的词共现信息，又注重原始短文本中的稀疏信息，从而更好挖掘出短文本中的主题信息。


\subsection{短文本主题模型的模型增强方法}
模型稀疏性增强的目标是使模型学习到更具语义明确和清晰的潜在表达，以在一定程度上缓解短文本词共现信息的稀疏问题。本研究主要集中在以下两个主要方面进行研究：稀疏主题模型的设计以及如何利用神经网络实现稀疏主题模型的变分推断算法。具体的研究内容如下：

（1）\textbf{稀疏主题模型设计：}为了增强文本表示的稀疏性，一种直接的方法是生成峰值分布，使每个短文本只关注少数几个主题。为实施表达的稀疏性约束，往往通过引入了额外的正则化约束项或额外的先验变量来实现。然而，在实际应用中，由于神经网络的主要优化算法是梯度反传算法。而主题模型涉及到分布的采样，并不是所有的分布采样过程都能够得到有效的梯度计算。为了解决这个问题，将研究适合的稀疏先验，与经典的主题模型结合以使后续的深度学习方法能够进行推断。

（2）\textbf{基于非均值场的变分推断算法：}神经主题模型通常使用变分自编码器得到模型的参数估计然而，当引入额外的变量时，这些模型普遍基于均值场理论，假设隐变量之间具有强独立性，以降低其复杂的理论推导过程。但在实际的应用过程中，隐变量之间存在着一定的关系。为了简化推导过程而忽略隐变量之间的关系，将难以得到高质量的主题。针对以上问题，本论文将研究非均值场假设的神经变分推断算法，通过使用神经网络来建模稀疏主题模型的生成过程，从而简化复杂的推断过程。

\section{论文组织结构}
本文分为五章，具体组织安排如下：

第1章介绍了短文本主题模型的研究背景及意义、短文本主题模型的研究现状，分析了现有短文本主题模型存在的问题和挑战，介绍说明了本文的研究目标、研究内容与核心贡献。

第2章介绍了相关技术与理论，包括主题模型的概述以及经典的适用于常规文本的LDA模型和适用于短文本的DMM模型，最后介绍了吉布斯采样算法以及基于变分自编码器的主题模型。

第3章介绍了一种短文本数据增强的方法，基于引导的短文本扩充方法，该方法将短文本扩充为（伪）长文本。同时提出了提出一种将伪长文本与短文本视为成对数据的主题模型TPTM。

第4章介绍了一种基于稀疏性增强的短文本主题模型增强方法，基于非均值场推理的神经稀疏主题建模方法 SpareNTM。

第5章介绍了本论文的研究内容总结，并且对本文研究内容做出展望。
