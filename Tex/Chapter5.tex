\chapter{总结与展望}
短文本是一种非常常见的文本类型，特别是随着互联网的发展，网络上出现了大量短文本数据集，比如微博、新闻标题、商品或图片的描述、评论、问答网站的问题等等。和常规文本不同，这些短文本的长度非常有限。要挖掘这些短文本中的信息，最常用的一类方法就是主题模型。主题模型首先构造了一个隐变量:主题，并通过文本采样生成主题，再由主题采样生成词语。主题本身就成为对文本内容的归纳总结，主题和词语的关系，也让主题成为一个语义概念，人们就可以通过主题来理解文本内容。只是，传统的主题模型只适用于常规长度的文本，对于短文本集，并不能取得很好的效果。这是因为短文本集中词语共现信息不足，造成词语的共现矩阵是稀疏的。而主题模型需要利用共现矩阵产生主题稀疏的共现矩阵必然让算法的结果变的很差。我们在充分的调研已有的工作之后，提出了新的短文本主题模型方法，解决主题模型在短文本上效果不佳的问题。本文主要介绍了两个研究内容，说明了如何通过这两个研究，不断提高主题模型在短文本集上的效果。在本章中，首先对所做的主要研究工作进行了总结，归纳了创新点，然后对未来的工作进行了展望。

\section{主要贡献}

1、许多研究针对主题模型提出了各种策略和方法来解决短文本数据的稀疏性问题。一个常用的类策略是通过构建一个辅助的常规文本集， 用迁移学习的方式，先从常规长度的文本中得到语义信息，这种语义信息通常以词嵌入的形式迁移到短文本主题模型中。然而，在这类算法中，错误信息往往并来自于辅助文本集和短文本集的内容不匹配，从辅助文本集得到的语义信息并不能够代表短文本集的语义信息。此外，构建辅助文本集的直接方法是从短文本自身入手，对其扩充而达到数据增强的目的，从而增加文档层面的词共现信息。但现有的文本扩充方法无法保留文本原来的语义信息，即扩充之后的“伪”长文与原来的短文之间存在语义不一致的问题。

为了解决以上问题，本文提出了一种数据增强的新方法，将每个短文基于大规模语言模型扩充为一个（伪）长文本，解决了以往短文本集和伪长文本集在语义上不一致的问题。与以往大多数工作中使用传统主题模型对扩充后得到的长文本进行主题挖掘不同，本论文研究了一个将伪长文本与原始的短文联合建模的主题模型，完善了数据增强之后的主题建模方法。

2、一些主题模型针对短文本的稀疏性问题引入了额外的正则化约束项或稀疏先验分布。在传统的非深度学习框架下，这些模型的推导求解非常复杂。而这些主题模型中潜在分布的参数估计往往是通过吉布斯采样来学习的。然而，这种推理方式不够灵活，且对于大型语料库而言计算代价高昂。基于神经网络（如变分自编码器）的主题模型可以加快推理速度，但这些神经主题模型更多地针对长文本设计，缺乏文本稀疏性相关的研究。

为了解决以上问题，本论文提出研究神经稀疏主题建模方法，弥补了神经主题模型在稀疏性建模上的不足。同时，突破了自编码器变分推断中常用的均值场假设，首次利用变分自编码器实现基于非均值场推断的主题模型，提高了模型的表达能力，更好地捕捉到文本集的主题结构。

\section{未来展望}
尽管，在短文本主题挖掘领域，本文提出了一系列解决方法和新模型，在该领域中仍然还有许多待探索和解决的关键问题。未来 的研究可能有以下几个方向。

1、本文第三章的方法是将大型语言模型的知识转移到短文本主题建模中，以此来改进挖掘出的潜在主题的质量。然而，本文在基于指导的文本扩充中所使用的指令提示可能是随意和简单的，可能会导致扩充得到的伪文档质量不高。因此如何在目标数据集上对大规模语言模型进行微调以获得更适合于主题建模的伪文档是一个可能研究方向。此外，在本文中，数据增强的方法与主题建模的方法是分开的，需要探讨的是，是否有可能仅依靠大规模语言模型，比如直接使用原始短文本作为大模型的输入，来有效地完成主题建模任务，而不需要额外的数据处理或算法支持。

2、在第四章本文定义的基于非均值场假设的变分分布中，我们更关注两个全局变量“文档-主题分布”和“主题选择器”之间的相互影响，而忽略了“主题选择器”各个分量之间的依赖关系。已有研究提出了一些二元隐变量（稀疏编码）的非均值场变分神经网络。尽管这些模型只用了这一稀疏编码作为数据的隐变量，一个可以参考的方向就是利用他们的方法对“主题选择器”的依赖关系进行建模，从而进一步提高本文模型的表达能力。